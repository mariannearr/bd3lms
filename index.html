<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="BAM-DLMs">
  <meta name="twitter:description" content="BAM-DLMs">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="block-autoregressive, discrete, masked, diffusion, language models, BAM-DLM, BAM-DLMs">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Block-Autoregressive Masked Diffusion Language Models for Improved Likelihoods & Flexible-Length Generation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Block-Autoregressive Masked Diffusion Language Models for Improved Likelihoods & Flexible-Length Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mariannearriola.github.io/" target="_blank">Marianne Arriola</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://s-sahoo.com/" target="_blank">Subham Sahoo</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://skylion007.github.io/" target="_blank">Aaron Gokaslan</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://justinchiu.netlify.app/" target="_blank">Justin Chiu</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://zhihanyang2022.github.io/" target="_blank">Zhihan Yang</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="https://zhixuanqi.com/" target="_blank">Zhixuan Qi</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="https://hanjq17.github.io/" target="_blank">Jiaqi Han</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Cornell&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>2</sup>Cohere&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>3</sup>Stanford</span>
                  </div>

                  <div class="is-size-5 publication-flag" style="padding-top: 10px; padding-bottom: 10px;">
                    <b>ICLR 2025 Oral</b>
                  </div>
                  <span class="link-block">
                    <a href="<https://openreview.net/forum?id=tyEyYT267x>" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/kuleshov-group/bam-dlms" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                <span class="link-block">
                  <a href="https://huggingface.co/collections/kuleshov-group/bam-dlms-67be95f81b96b15fec50d53f" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf.png" alt="Hugging Face" style="width: 30px; height: 25px; vertical-align: middle;">
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser is-max-desktop">
  <div class="container">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Autoregressive decoding: 
        <span style="color: green; font-weight: bold;">&#10004; High quality</span> 
        <span style="color: green; font-weight: bold;">&#10004; Arbitrary lengths</span>
        <span style="color: red; font-weight: bold;">&#10060; Not parallelizable</span> 
      </h2>
      <img src="static/images/text_ar.gif" style="padding-bottom: 60px;">

      <h2 class="subtitle has-text-centered">
        Diffusion decoding: 
        <span style="color: red; font-weight: bold;">&#10060; Lower quality</span> 
        <span style="color: red; font-weight: bold;">&#10060; Fixed length</span> 
        <span style="color: green; font-weight: bold;">&#10004; Parallelizable</span> 
      </h2>
      <img src="static/images/text_mdlm.gif" style="padding-bottom: 60px;">
      <h2 class="subtitle has-text-centered" style="padding-top: 20px;">
        Block-autoregressive diffusion decoding: 
        <span style="color: green; font-weight: bold;">&#10004; High quality</span> 
        <span style="color: green; font-weight: bold;">&#10004; Arbitrary lengths</span> 
        <span style="color: green; font-weight: bold;">&#10004; Parallelizable</span> 
      </h2>
      <img src="static/images/text_sar.gif" style="padding-bottom: 20px;">
      </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion language models offer unique benefits over autoregressive (AR) models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block-autoregressive (BAR) diffusion models that interpolate between discrete denoising diffusion and autoregressive models. We propose a recipe for building effective BAR models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. BAR models overcome key limitations of diffusion language models, setting a new state-of-the-art performance on language modeling benchmarks and enabling generation of arbitrary-length sequences.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- AR VS DIFFUSION -->
<section class="section" id="SEC1">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Autoregressive vs. Diffusion Language Models</h2>
      <p>
        In the language modeling task, we have a sequence of \( L \) tokens \( \mathbf{x} = (x^1, \dots, x^L ) \) drawn from the data distribution \( q(\mathbf{x}) \). We aim to fit a model \( p_\theta(\mathbf{x}) \) of \( q \).
      </p>
        Autoregressive models define a factorized distribution of the form:
      </p>
      <p>
        \[ \log p_\theta(\mathbf{x}) = \sum_{i=1}^L \log p_\theta(\mathbf{x}^i \mid \mathbf{x}^{\lt i}) \]
      </p>
      <p>However, the sequential dependencies between tokens require that AR sampling is restricted to \( L \) sampling steps, which may be slow for long sequences.</p>
      <p>Diffusion models overcome this limitation by modeling tokens independently, admitting parallel generation. Diffusion models instead fit a model to undo a forward corruption process \( q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \text{Cat}(\mathbf{x_t} ; Q_t \mathbf{x}_{t-1} ) \) using transition matrices \( Q_t \). D3PM (<a href="https://arxiv.org/abs/2107.03006">Austin et. al</a>) defines this as</p>
      <p>
        \[ p_\theta(\mathbf{x}_s | \mathbf{x}_t) = \sum_{\mathbf{x}} q(\mathbf{x}_s | \mathbf{x}_t, \mathbf{x}) p_\theta(\mathbf{x} | \mathbf{x}_t)   \]
      </p>
      <p>where the denoising base model \( p_\theta(\mathbf{x} | \mathbf{x}_t) \) predicts clean tokens \( \mathbf{x} \) given noised tokens \( \mathbf{x}_t \). However, the diffusion objective minimizes a bound on the likelihood. As a result, diffusion models lag in terms of likelihood and sample quality. Furthermore, diffusion models are restricted to generate fixed length sequences.</p>
    </div>
  </div>
</section>

<!-- BAM-DLMs -->
<style>
      /* .link-block {
          display: inline-block;
          text-align: center;
          margin: 4px;
          font-size: 1.1em;
        }

        .link-block .external-link.button {
          font-size: 0.9em;
        }

        .link-block .icon {
          font-size: 1.2em;
        } */

      .is-max-desktop {
          max-width: 50%; /* Increase the width */
          width: 50%;
      }
      .section {
            padding-left: 5%;
            padding-right: 5%;
        }
        .hero-body {
          display: flex;
          flex-direction: column; /* Captions and figure container are in a vertical stack */
          align-items: center;
      }
      .subtitle.has-text-centered {
          font-size: 1.4em; /* Increase font size */
      }

        p {
          font-size: 18px;
        }

      

        .figure-caption-container {
            display: flex;
            flex-direction: column; /* Captions and figure container are in a vertical stack */
            align-items: center;
            gap: 20px; /* Spacing between figure and color captions */
        }

        .figure-content-container {
            display: flex;
            flex-direction: row; /* Figure and color boxes are side by side */
            align-items: center;
            gap: 20px;
        }

        .figure-container {
            flex-shrink: 0; /* Prevents figure from shrinking */
        }

        .captions {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: flex-start;
        }

        .caption {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
            font-size: 16px;
        }
        .title {
          padding-bottom: 10px;
          font-size: 30px;
        }
  .color-box {
      display: inline-block;
      width: 20px;
      height: 20px;
      margin-right: 5px;
      vertical-align: middle;
  }
  .orange { background-color: #ff9900; }
  .light-orange { background-color: #FFDAB9; }
  .blue { background-color: #30beee; }
  .dark-green { background-color: #006400; }
  .green { background-color: #228B22; }
  .light-green { background-color: #90EE90; }
</style>
<style>
  table {
    width: 50%; /* Reduce overall table width */
    margin: 10px auto;
    border-collapse: collapse;
    font-size: 18px; /* Reduce font size */
  }

  th, td {
    padding: 4px 6px; /* Reduce padding */
    border: 1px solid #ddd; /* Keep border for readability */
    text-align: center;
  }

  th {
    background-color: #f2f2f2;
  }

  .figure-caption {
    font-size: 18px; /* Reduce caption font size */
  }
</style>


<section class="section" id="SEC2">
  <div class="container is-max-desktop">
    <div class="content is-medium">
      <h2 class="title">BAM-DLMs: Block Autoregressive Masked Diffusion Language Models</h2>
      <p>We combine modeling paradigms to enjoy better likelihoods & flexible-length generation from autoregressive models, as well as fast & parallel generation from diffusion models.</p> 
      <h4 class="subtitle">Block-autoregressive likelihood</h4>
      <p>We propose a modeling framework that autoregressively models blocks of tokens and performs diffusion within each block. Our likelihood factorizes over \( B \) blocks of length \( L' \) as</p>
      <p>
        \[ \log p_\theta (\mathbf{x}) = \sum_{b=1}^B \log p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \]
      </p>
      <p>Each \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) is modeled using discrete diffusion ELBO over a block of \( L' \) tokens. We obtain a principled learning objective \( \mathcal{L}_\text{BAR}(\mathbf{x}, \theta) \) by optimizing the following likelihood bound:</p>
      <p>
        \[ \log p_\theta(\mathbf{x}) \geq \mathcal{L}_\text{BAR}(\mathbf{x}, \theta) := \sum_{b=1}^{B} \mathcal{L}(\mathbf{x}^b, \mathbf{x}^{\lt b}, \theta), \]
      </p>
      We model the per-block likelihood under a simple discrete diffusion parameterization (<a href="https://arxiv.org/abs/2406.07524">Sahoo et. al</a>, <a href="https://arxiv.org/abs/2406.04329">Shi et. al</a>, <a href="https://arxiv.org/abs/2406.03736">Ou et. al</a>). Our final objective becomes a sum of weighted cross-entropy terms:
      <p>
        \[ \mathcal{L}_\text{BAR}(\mathbf{x}, \theta) :=  \sum_{b=1}^{B} \mathbb{E}_{t_b \sim (0, 1]} \mathbb{E}_{q} \frac{1}{t_b} \log p_\theta(\mathbf{x}^b | \mathbf{x}_{t_b}^b, \mathbf{x}^{\lt b}) \]
      </p>
      <h4 class="subtitle">Efficient training using a custom attention algorithm</h4>
      <p> Ideally, we wish to model all conditional terms \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) for \( B \) blocks in one forward pass of the model \( \mathbf{x}_\theta \). However, observe that denoising \( \mathbf{x}_t^b \) requires a forward pass on this noisy input, while denoising the next block requires running the model \( \mathbf{x}_\theta \) on the clean version \( \mathbf{x}^b \).</p>
      <p>We propose to efficiently model all \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) in a vectorized fashion by proposing a custom attention mask. This mask ensures that each conditional term is modeled using a special attention pattern, where each token in \( \mathbf{x}^b \) self-attends to the noised block and cross-attends to the clean context \( \mathbf{x}^{\lt b} \). We update \( \mathbf{x}^{\lt b} \) needed for this cross-attention using block-causal attention. </p>
      <section class="hero is-small">
        <div class="hero-body has-text-centered">
            <div class="figure-caption-container">
                <!-- Figure Caption Above the Figure and Color-Coded Labels -->
                <!-- Container for figure and color-coded captions next to each other -->
                <div class="figure-caption">
                  <i>Custom attention mask to model \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) for \( B \) blocks in one forward pass.</i>
              </div>
                <div class="figure-content-container">
                    <div class="figure-container">
                        <!-- Figure/Image -->
                        <img src="static/images/mask.png" alt="mask" style="width: 375px;">
                    </div>
                    <div class="captions">
                        <!-- Labels/Captions to the right of the figure -->
                        <div class="caption">
                            <div class="color-box orange"></div>
                            <span>Self-attention mask within noised blocks \( \mathbf{x}_t^b \)</span>
                        </div>
                        <div class="caption">
                            <div class="color-box blue"></div>
                            <span>Cross-attention mask for conditional context \( \mathbf{x}^{\lt b} \)</span>
                        </div>
                        <div class="caption">
                            <div class="color-box dark-green"></div>
                            <span>Block-causal attention mask to update \( \mathbf{x} \)</span>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </section>
    </div>
  </div>
</section>

<!-- GAP -->
<section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Understanding Likelihood Gaps Between Diffusion and AR Models</h2>
      <h4 class="subtitle">Case Study: Single Token Generation</h4>
      <p>Our BAR parameterization is equivalent in expectation to the autoregressive NLL in the limiting case where \( L'=1 \). Surprisingly, we find a two point perplexity gap between our BAR model for \( L'=1 \) and AR when training both models on the LM1B dataset. We identify high training variance of the diffusion objective as responsible for the perplexity gap.</p>
      <section class="hero is-small">
        <div class="hero-body">
        <img src="static/images/bs1.png" alt="mask" style="width:80%; padding-bottom: 10px;">
        <div class="figure-caption">
          <i>Training under the discrete diffusion ELBO suffers from high variance.</i>
      </div>
        </div>
      </section>
      <h4 class="subtitle">Diffusion Gap from High Variance Training</h4>
        <p>Intuitively, if the sampled masking rate \( t \sim \mathcal{U}[0, 1] \) is too low, reconstructing \( \mathbf{x} \) is easy, which does not provide a useful learning signal. If we mask everything, the optimal reconstruction are the marginals of each token in the data distribution, which is easy to learn, and again not useful.</p>
        We seek to find noise schedules that minimize training variance caused by the diffusion objective and further reduce the perplexity gap.</p>
      </div>
  </div>
</section>

<!-- CLIPPED SCHEDULES -->
<section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Data-Driven Clipped Schedules for Low-Variance Gradients</h2>
      <p>To avoid masking rates that cause high-variance training, we train BAM-DLMs under "clipped" masking rates \( t \sim \mathcal{U}[\beta, \omega] \) for \( 0 \leq \beta, \omega \leq 1 \). By reducing the training variance, we improve likelihoods when we evaluate under uniformly sampled mask rates.</p>

      <p>As the optimal mask rates may differ depending on the block size \(L'\), we adaptively learn \( \beta, \omega \) during training. In practice, we do so using a grid search during every validation step, after 5K gradient updates, to optimize \(\min_{\beta, \omega} \text{Var}_{\mathbf{X}, t} \left[ \mathcal{L}_{\text{BAR}}(\theta, \beta, \omega; \mathbf{X}) \right] \). </p>
      
      <p>Below, we show that optimizing the noise schedule per block size reduces the variance of the loss estimator and achieves the best perplexities compared to alternative schedules.</p>
    </div>

    <table style="width: 70%; margin: 20px auto; border-collapse: collapse; font-size: 19px; text-align: center;">
      <div class="figure-caption">
        <center>
        <i style="font-size: 19px;">Effect of training under different noise schedules on perplexity (PPL  ↓) on LM1B. All models are finetuned for 50K steps and are evaluated under uniformly sampled mask rates. For our clipped schedules, we compare the optimized clipping rates for \( L'=4, 16 \).</i>
        </center>
      </div>
      <tr>
          <th style="padding: 6px 10px; border-bottom: 1px solid #ddd; background-color: #f2f2f2;">BAM-DLMs</th>
          <th style="padding: 6px 10px; border-bottom: 1px solid #ddd; background-color: #f2f2f2;">Noise schedule</th>
          <th style="padding: 6px 10px; border-bottom: 1px solid #ddd; background-color: #f2f2f2;">PPL</th>
          <th style="padding: 6px 10px; border-bottom: 1px solid #ddd; background-color: #f2f2f2;">Var. ELBO</th>
      </tr>
  
      <!-- Multi-row for L' = 4 -->
      <tr>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold; vertical-align: middle;" rowspan="4">L' = 4</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">Linear <em>t</em> ~ <em>U</em>[0, 1]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">30.18</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">23.45</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">Clipped <em>t</em> ~ <em>U</em>[0.45, 0.95]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">29.21</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">6.24</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">Clipped <em>t</em> ~ <em>U</em>[0.3, 0.8]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">29.38</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">10.33</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: 2px solid #ddd;">Logarithmic</td>
          <td style="padding: 6px 10px; border-bottom: 2px solid #ddd;">30.36</td>
          <td style="padding: 6px 10px; border-bottom: 2px solid #ddd;">23.53</td>
      </tr>
  
      <!-- Multi-row for L' = 16 -->
      <tr>
          <td style="padding: 6px 10px; border-bottom: none; font-weight: bold; vertical-align: middle;" rowspan="4">L' = 16</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">Linear <em>t</em> ~ <em>U</em>[0, 1]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">31.72</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">7.62</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">Clipped <em>t</em> ~ <em>U</em>[0.45, 0.95]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">31.42</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd;">3.60</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">Clipped linear <em>t</em> ~ <em>U</em>[0.3, 0.8]</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">31.12</td>
          <td style="padding: 6px 10px; border-bottom: 1px solid #ddd; font-weight: bold;">3.58</td>
      </tr>
      <tr>
          <td style="padding: 6px 10px; border-bottom: none;">Cosine</td>
          <td style="padding: 6px 10px; border-bottom: none;">31.41</td>
          <td style="padding: 6px 10px; border-bottom: none;">13.00</td>
      </tr>
  </table>
  </div>
</section>

<!-- RESULTS -->
<section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Results</h2>
      <h4 class="subtitle">Likelihood Evaluation</h4>
      <p>BAM-DLMs achieve state-of-the-art likelihoods among diffusion models. As shown below, BAM-DLMs interpolate between diffusion and autoregressive likelihoods by tuning the block length \( L' \). </p>
      <style>
        table {
          border-collapse: collapse;
          width: 100%;
        }
        th, td {
          padding: 8px;
          text-align: left;
        }
        /* Top border for category headers, without bottom border */
        tr.category-row td {
          border-top: 2px solid #000;
          border-bottom: none;
        }
        /* Bold top border for the block-autoregressive section */
        tr.block-autoregressive-row td {
          border-top: 2px solid #000; /* Bold border */
          border-bottom: none;
        }
        /* Remove bottom borders for grouped models */
        tr.grouped-models td {
          border-bottom: none;
        }
        /* Light bottom border specifically for the last row in block-autoregressive */
        tr.final-row td {
          border-bottom: 2px solid #000; /* Light border */
        }
        /* Make BAM-DLMs invisible */
        .invisible {
          visibility: hidden; /* Makes text invisible but preserves space */
        }
      </style>
      <style>
        .compact-table {
            max-width: 400px;  /* Adjust width as needed */
            width: 100%;
            margin: auto;
            border-collapse: collapse;
        }
        .compact-table th, .compact-table td {
            padding: 5px 10px;  /* Reduce padding */
            text-align: left;
            white-space: nowrap; /* Prevent wrapping */
        }
        .compact-table th {
            background-color: #f2f2f2; /* Light gray header */
        }
    </style>
      <table class="compact-table">
            <center>
                <i>Test perplexities (PPL; ↓) on OWT for models trained for 262B tokens.</i>
            </center>
        </div>
        <thead>
            <tr>
                <th>Model</th>
                <th>PPL (↓)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>AR</td>
                <td>17.54</td>
            </tr>
            <tr>
                <td>SEDD</td>
                <td>≤ 24.10</td>
            </tr>
            <tr>
                <td>MDLM</td>
                <td>≤ 22.98</td>
            </tr>
            <tr>
                <td><strong>BAM-DLMs</strong> L' = 16</td>
                <td>≤ 22.27</td>
            </tr>
            <tr>
                <td><span class="invisible">BAM-DLMs</span> L' = 8</td>
                <td>≤ 21.68</td>
            </tr>
            <tr>
                <td><span class="invisible">BAM-DLMs</span> L' = 4</td>
                <td><strong>≤ 20.73</strong></td>
            </tr>
        </tbody>
    </table>
    
      <h4 class="subtitle">Arbitrary-length sequence generation</h4>
      <p>One key drawback of many existing diffusion language models is that they cannot generate full-length documents that are longer than the length of the output context chosen at training time. For example, OpenWebText contains documents up to 131K tokens, whereas discrete diffusion model SEDD (<a href="https://arxiv.org/abs/2310.16834">Lou et. al</a>) is restricted to generate 1024 tokens. Below, we show that BAM-DLMs can generate variable-length documents by decoding an arbitrary number of blocks.</p>
      
      <table class="compact-table">
        <div class="figure-caption">
        <center>
          <i>Generation length statistics from sampling 2K documents from models trained on OWT. For diffusion models, we use T=1K diffusion steps.</i>
        </center>
      </div>
        <thead>
          <tr>
            <th></th>
            <th>Median # tokens</th>
            <th>Max # tokens</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OWT train set</td>
            <td>717</td>
            <td>131K</td>
          </tr>
          <tr>
            <td>AR</td>
            <td>2053</td>
            <td>2895</td>
          </tr>
          <tr>
            <td>SEDD</td>
            <td>1013</td>
            <td>1024</td>
          </tr>
          <tr>
            <td><strong>BAM-DLM</strong> L'=16</td>
            <td>850</td>
            <td>7328</td>
          </tr>
        </tbody>
      </table>
      
      
      <p> We assess the sample quality of BAM-DLMs on variable-length sequences. In the below table, we measure the generative perplexity of sampled sequences of lengths L=1024,2048 tokens under GPT2-Large. For MDLM, we use their block-autoregressive decoding technique (which does not feature block-autoregressive training as in BAM-DLMs) for L=2048. We also compare to SSD-LM (<a href="https://arxiv.org/abs/2210.17432">Han et. al</a>) an alternative semi-autoregressive formulation that performs Gaussian diffusion over word embeddings. Unlike SSD-LM, BAM-DLMs support likelihood estimation and applies discrete noise.</p>

      <p>BAM-DLMs achieve the best generative perplexities compared to all previous diffusion methods. Compared to SSD-LM, our discrete approach yields samples with improved generative perplexity using an order of magnitude fewer generation steps.</p>


      <!-- <p>We also compare with SSD-LM (<a href="https://arxiv.org/abs/2210.17432">Han et. al</a>), an alternative block-autoregressive formulation that performs Gaussian diffusion over word embeddings, as in other works such as Diffusion-LM (<a href="https://arxiv.org/abs/2205.14217">Li et. al</a>). Our approach instead applies discrete noise and can be seen as the analogous extension of <a href="https://arxiv.org/abs/2107.03006">Austin et. al</a>. Unlike BAM-DLMs, SSD-LM does not support likelihood estimation. Thus, to compare the models, we generated unconditionally sequences of lengths 1024 and 2048. Our discrete approach yields samples with improved generative perplexity using an order magnitude fewer generation steps.</p> -->
        
      <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            border: 1px solid black;
            padding: 8px;
            text-align: center; /* Center align all table entries */
            vertical-align: middle;
        }
        th {
            background-color: #f2f2f2;
        }
        caption {
            text-align: center;
            font-weight: bold;
            margin-bottom: 10px;
        }
    </style>
    <table style="border-collapse: collapse; width: 100%;">
      <div class="figure-caption">
        <center>
          <i>Generative perplexity (Gen. PPL; ↓) of 50 variable-length samples. All models are trained on OWT with a context length L = 1024 and use nucleus sampling.</i>
        </center>
      </div>
      <thead>
        <tr>
          <th>Category</th>
          <th>Model</th>
          <th colspan="2"><center>L = 1024</center></th>
          <th colspan="2"><center>L = 2048</center></th>
        </tr>
        <tr>
          <th style="padding: 8px;"></th>
          <th style="padding: 8px;"></th>
          <th>Gen. PPL (↓)</th>
          <th>NFEs</th>
          <th>Gen. PPL (↓)</th>
          <th>NFEs</th>
        </tr>
      </thead>
      <tbody>
        <!-- Autoregressive Section -->
        <tr>
          <td style="border-bottom: 2px solid black; text-align: left; padding: 8px;"><i>Autoregressive</i></td>
          <td style="border-bottom: 2px solid black; padding: 8px;">AR</td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>14.5</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>1K</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>13.1</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>2K</center></td>
        </tr>
    
        <!-- Diffusion Section -->
        <tr>
          <td style="border-bottom: 2px solid black; text-align: left; padding: 8px;" rowspan="2"><i>Diffusion</i></td>
          <td style="padding: 8px;">SEDD</td>
          <td style="padding: 8px;"><center>53.2</center></td>
          <td style="padding: 8px;"><center>1K</center></td>
          <td style="padding: 8px;"><center>-</center></td>
          <td style="padding: 8px;"><center>-</center></td>
        </tr>
        <tr>
          <td style="border-bottom: 2px solid black; padding: 8px;">MDLM</td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>46.3</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>1K</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>38.1</center></td>
          <td style="border-bottom: 2px solid black; padding: 8px;"><center>2K</center></td>
        </tr>
        
        <!-- Block-autoregressive Section -->
        <tr>
          <td style="border-bottom: none; text-align: left; padding: 8px;" rowspan="5"><i>Block-autoregressive</i></td>
          <td style="border-bottom: none; padding: 8px;">SSD-LM L' = 25</td>
          <td style="border-bottom: none; padding: 8px;"><center>36.9</center></td>
          <td style="border-bottom: none; padding: 8px;"><center>40K</center></td>
          <td style="border-bottom: none; padding: 8px;"><center>35.2</center></td>
          <td style="border-bottom: none; padding: 8px;"><center>80K</center></td>
        </tr>
        <tr>
          <td style="padding: 8px;"></td>
          <td style="border-top: none; padding: 8px;"><center>325.8</center></td>
          <td style="border-top: none; padding: 8px;"><center>1K</center></td>
          <td style="border-top: none; padding: 8px;"><center>278.1</center></td>
          <td style="border-top: none; padding: 8px;"><center>2K</center></td>
        </tr>
        <tr>
          <td style="border-top: 1px solid #000000; padding: 8px;border-bottom: none;"><strong>BAM-DLMs</strong> L' = 16</td>
          <td style="padding: 8px;border-bottom: none;"><center>37.4</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>1K</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>36.5</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>2K</center></td>
        </tr>
        <tr>
          <td style="padding: 8px;border-bottom: none;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; L' = 8</td>
          <td style="padding: 8px;border-bottom: none;"><center>34.9</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>1K</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>35.5</center></td>
          <td style="padding: 8px;border-bottom: none;"><center>2K</center></td>
        </tr>
        <tr>
          <td style="padding: 8px;" class="highlight">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L' = 4</td>
          <td style="padding: 8px;" class="highlight"><center><strong>28.5</strong></center></td>
          <td style="padding: 8px;"><center>1K</center></td>
          <td style="padding: 8px;" class="highlight"><center><strong>27.9</strong></center></td>
          <td style="padding: 8px;"><center>2K</center></td>
        </tr>
        
      </tbody>
    </table>
    
    
    
    </div>
  </div>
</section>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{
          arriola2025interpolating,
          title={Interpolating Autoregressive and Discrete Denoising Diffusion Language Models},
          author={Marianne Arriola and Aaron Gokaslan and Justin T Chiu and Jiaqi Han and Zhihan Yang and Zhixuan Qi and Subham Sekhar Sahoo and Volodymyr Kuleshov},
          booktitle={The Thirteenth International Conference on Learning Representations},
          year={2025},
          url={https://openreview.net/forum?id=tyEyYT267x}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
